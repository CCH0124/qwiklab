---
layout: post
title: Baseline Data, ML, AI- 03
date: 2020-09-05
excerpt: "Dataflow: Qwik Start"
tags: [qwiklab, ML, google]
comments: true
---

學習如何使用 [Google Cloud Dataflow 模板](https://cloud.google.com/dataflow/docs/templates/provided-templates)之一創建串流傳輸管道。更具體說，將使用 `Cloud Pub/Sub to BigQuery` 模板，該模板從 `Pub/Sub topic` 讀取以 `JSON` 編寫的訊息，並將其推送到 `BigQuery` 表。可以在[此處](https://cloud.google.com/dataflow/docs/templates/provided-templates#cloudpubsubtobigquery)找到該模板的文檔。

## Create a Cloud BigQuery Dataset and Table Using Cloud Shell
創建一個 `BigQuery` 數據集和表格。

以下指令創建名為 `taxiderides` 的數據集：
```shell
bq mk taxirides
```
輸出會是以下
```shell
Dataset '<myprojectid:taxirides>' successfully created
```


已經創建了數據集，接下來的步驟將使用它來實例化 `BigQuery` 表。運行以下指令來這樣做：

```shell
bq mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
```

輸出會類似於
```shell
Table 'myprojectid:taxirides.realtime' successfully created
```

[BigQuery 指令操作文件](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)

### Create a storage bucket
我們創建一個儲存桶

```shell
export BUCKET_NAME=<your-unique-name>
gsutil mb gs://$BUCKET_NAME/
```

## Create a Cloud BigQuery Dataset and Table Using the Cloud Console

這邊是以圖形介面操作上一章節的事。

在左側選單的"Big Data"部分中，點擊 `BigQuery`。點擊左邊選單的專案名稱，點擊右側 `CREATE DATASET`，輸入 ID `taxirides`。

![](https://cdn.qwiklabs.com/bRb1qVZ2z54AcpeDuFy%2B85MuvYJY4AVLbyNSjyGBITw%3D)


在左側控制台中的項目 ID 下方看到 `taxirides` 數據集點擊它，然後在控制台右側選擇 `CREATE TABLE`。在 `Destination table` 輸入中，輸入 `realtime`。

在`Schema`下，切換`Edit as text`並輸入以下內容：

```shell
ride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,
meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer
```

其網頁內容如下

![](https://cdn.qwiklabs.com/gYNzUZhtzX8OAdnoYoNY31oG6lZWz97RxhclS4pX9uM%3D)

### Create a storage bucket

`Storage > Browser > Create bucket`，給儲存桶唯一名稱並建立

![](https://cdn.qwiklabs.com/kOdGE5oSj2FLlqRFgHSobVa8dbytB9uLsahqTsoSuYY%3D)

## Run the Pipeline

在選單中選擇 `Dataflow`，點擊 `+ Create job from template`，輸入 `Cloud Dataflow` 的 `Job name`，在 `Cloud Dataflow Template`下，選擇`Cloud Pub/Sub Topic to BigQuery` 模板。

在 `Cloud Pub/Sub input topic` 輸入主題下，輸入：
```shell
projects/pubsub-public-data/topics/taxirides-realtime
```

在 `BigQuery` 輸出表下，輸入創建的表的名稱：
```shell
<myprojectid>:taxirides.realtime
```

將儲存桶添加為 `Temporary Location`

```shell
gs://Your_Bucket_Name/temp
```


上面流程以 GUI 呈現如下
![](https://cdn.qwiklabs.com/kJiAXaVD5HH1IYxC0CWj%2BmR%2B9CqSFNJm3aDa%2Fm%2FEIco%3D)

最後點擊 `Run job`，之後將看到資源相關構建並可以使用，再跳到 `BigQuery` 查看寫入數據，當 `BigQuery UI` 打開時，將看到在項目名稱下添加的 `taxirides`，並在其下 `realtime` 顯示：

![](https://cdn.qwiklabs.com/qO%2BbkqpwX5%2Begoc38sfCCc1Q3OmjzMYVg%2B8CQgmEnUA%3D)


其會有流程圖的GUI
![](https://i.imgur.com/CSX2t0W.png)

## Submit a query
可以使用標準 `SQL` 進行查詢。在 `Query editor` 添加以下字段，最後在運行 `Run Query`

```sql
SELECT * FROM `myprojectid.taxirides.realtime` LIMIT 1000
```

查詢成功後會有以下結果

![](https://i.imgur.com/Eml9GAK.png)
