---
layout: post
title: Baseline Data, ML, AI- 05
date: 2020-09-05
excerpt: "Reinforcement Learning: Qwik Start"
tags: [qwiklab, ML, google]
comments: true
---

此實驗，直接在 `GCP` 上執行，但本人對 `RF` 沒什麼研究，所以程式碼部分待了解一些，這邊指放置 `RF` 介紹。

與機器學習研究的許多其他領域一樣，[強化學習（RL）](https://en.wikipedia.org/wiki/Reinforcement_learning)以驚人的速度發展。就像他們在其他研究領域所做的一樣，研究人員正在利用深度學習來獲得最新的成果。

特別是，強化學習在遊戲中的表現大大超過了以前的機器學習技術，在Atari上達到了人類水平甚至世界上最好的表現，擊敗了人類圍棋冠軍，並且在諸如Starcraft II之類的更困難的遊戲中顯示出令人鼓舞的結果。

在本實驗中，您將通過構建一個簡單的遊戲來學習強化學習的基礎，該遊戲是根據OpenAI Gym提供的示例進行建模的 。


## Reinforcement learning 101
強化學習（RL）是機器學習的一種形式，通過這種形式，代理可以在環境中採取措施，以在此步驟序列中最大化給定的目標（獎勵）。與更傳統的監督學習技術不同，每個數據點均未標記，並且代理只能訪問*稀疏*獎勵。

雖然RL的歷史可以追溯到1950年代，並且有很多RL算法，但是2種易於實現但功能強大的深度RL算法最近吸引了很多人：deep Q-network（DQN）和deep deterministic policy gradient（DDPG）。在本節中，我們簡要介紹基於它們的算法和變體。

![](https://cdn.qwiklabs.com/cDBDy0wLYFlwkAnG0PrdbCg7UAEngRYH%2BORdWseL14A%3D "A conceptual process diagram of the Reinforcement Learning problem")

Deep Q-network（DQN）是Google Deepmind小組於2015年在《Nature》[雜誌](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)上介紹的。受到深度學習在圖像識別領域取得成功的鼓舞，作者將 deep neural networks 整合到了 Q-Learning 中，並在 Atari 遊戲引擎模擬器中對其算法進行了測試，其中觀察空間的維數非常大。

deep neural networks 充當函數逼近器(approximator)，在給定特定輸入狀態的情況下，預測輸出 Q 值或採取動作的必要性。因此，DQN是一種基於值的方法：在訓練算法中，DQN 根據 Bellman 公式更新 Q 值，並且為避免擬合運動目標的困難，它採用了第二個 deep neural network 來估計目標值 。

在更實際的水平上，以下模型突出顯示了要在 GCP 上運行的 RL 作業的來源檔案、shell 和端點：

![](https://cdn.qwiklabs.com/FQvwxiTxO%2FJ5baJVEDsj0tKHG1hvn27YHmaa0FHFbS4%3D)



## Create an AI Platform Notebook

`AI Platform > Notebooks` 然後 `+ New Instance > TensorFlow 2.x > Without GPUs`，最後點擊 `OPEN JUPYTERLAB`。



















